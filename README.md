# My LLM Journey

Welcome to my repository documenting my journey to becoming a master of all LLMs. 
This repository contains projects, notes, and resources that I've used.

## About

This project is a personal documentation of my learning journey with LLMs. It covers various aspects of LLMs, including training, fine-tuning, deployment, and integration with other technologies.

## Projects

### Project 1: Fine-tuning Flan-T5 for Summarization
- **Description**: This project focuses on fine-tuning the Flan-T5-base model using the dialogsum dataset to create effective summarizations. I am exploring multiple approaches including instruct fine-tuning and PEFT (Prompt Engineering and Fine-Tuning) to enhance model performance and adaptability.
- **Technologies Used**: Flan-T5, DiagSumm, Hugging Face Transformers, Python.
- **[Link to Project](URL_to_project)**

## Learning Resources

In this section, I share resources that have been helpful to me. This includes books, articles, courses, and tutorials.

1. [Hugging Face Course](https://huggingface.co/course) - A comprehensive guide to using transformers and fine-tuning models.
2. [Generative AI with LLMs](https://www.coursera.org/learn/generative-ai-with-llms) - Great course about LLMs.
3. [Attention is all you need](https://arxiv.org/abs/1706.03762) - Introduction of the Transformers architecture (Megatron thanks you)


## Challenges and Learnings

- **Challenge 1**: My Gtx1070 8Gb takes her time to generate.
- **Challenge 2**:If you think a 1B model needs almost 24G to train, I'm never training my own pedro-t5 :( 

## Future Goals

- **Goal 1**: Get good PC. Nice!
- **Goal 2**: Deploy my own fine-tuned model, to take my spam calls.

Thank you for visiting my LLM journey repository!
